{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Extraction Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Levenshtein import distance as lev_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Basic Dimensions***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric: Overlap\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\frac{P(T_1 \\cap T_2)}{P(T_1 \\cup T_2)}\n",
    "$$\n",
    "\n",
    "Explanation: This metric measures the overlap when the tables are superimposed on one another by looking at the ratio of shared cells to total cells (i.e. intersection vs union). This then equally treats small tables that are off by small amounts and larger tables that are incorrect by proportionally similar counts. This simply measures dimension similarity between tables and does not look at their actual contents, just whether cells exist or not. This measure is symmetrical and ressembles the Jaccard similarity index.\n",
    "\n",
    "Scale:  $0 \\leq x \\leq 1$. It is similar to a similarity percentage in that a case with two tables of identical dimensions will return a value of 1 (100%). The closer it is to 1, the more alike the dimensions of the two tables are, while a value near 0 indicates very dissimilar dimensions. No table pair will ever return a value of exactly 0 since all tables will have at least one cell and thus be shared between the intersection of the two (unless no table is read)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(df1, df2):\n",
    "    # Find intersection area (mulitplication of dim mins)\n",
    "    xmin = min(df1.shape[0], df2.shape[0])\n",
    "    ymin = min(df1.shape[1], df2.shape[1])\n",
    "    intersection = xmin*ymin\n",
    "    # Find union area (multiplication of dim maxs minus multiplication of dim max-min diffs)\n",
    "    xmax = max(df1.shape[0], df2.shape[0])\n",
    "    ymax = max(df1.shape[1], df2.shape[1])\n",
    "    union = xmax*ymax - (xmax-xmin)*(ymax-ymin)\n",
    "    # Calc\n",
    "    overlap_pct = intersection / union\n",
    "    return round(overlap_pct, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Contents***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric: String Similarity\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "1 - \\frac{Lev(~ str(T_1) ~ , ~ str(T_2) ~)}{max(~len(str(T_1))~,~len(str(T_2))~)}\n",
    "$$\n",
    "\n",
    "Explanation: First, the tables are transformed into string values which are then compared by calculating the Levenshtein distance which measures the number of edits required to obtain one string when starting from the other. To account for larger strings requiring more edits, this is then divided by the length of the bigger of the two strings, giving us edits per character. The scale is then inverted by doing 1 minus this ratio. This measure is symmetrical.\n",
    "\n",
    "Scale:  $0 \\leq x \\leq 1$. The fraction on its own is similar to a distance metric where identical elements will return a value of zero while a value of 1 represents completely different variables. However, this measure takes its complement, meaning 1 minus the fraction/percentage (1-%). This is done to align the metric with all the other table extraction performance statistics which act as similarity percentages. Therefore, the closer it is to 1, the more alike the tables contents are, while a value near 0 indicates very dissimilar table strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_similarity(df1, df2):\n",
    "    # Transform tables to strings\n",
    "    str1 = df1.to_string()\n",
    "    str2 = df2.to_string()\n",
    "    # Calc\n",
    "    edits_per_char = lev_dist(str1, str2) / max(len(str1), len(str2))\n",
    "    return round(1-edits_per_char, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Structure***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric: Completeness\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\frac{\\text{\\# of completely identified elements}}{\\text{\\# of real elements}}\n",
    "$$\n",
    "\n",
    "Explanation: When an element gets split into multiple parts, the detected output elements are classified as incomplete. As a result, a completely identified column for example must contain all of its cells in the processed output table. So it essentially looks to capture missed and split table elements.\n",
    "\n",
    "Scale:  $0 \\leq x \\leq 1$. It should be interpreted as a percentile score where a score of 1 (or 100%) indicates a perfectly complete processed table that in each cell, contains ALL the items of the original table's respective cells. Meanwhile, a lower score near zero indicates the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeness(df1, df2):\n",
    "    # Transform tables to lists of lists\n",
    "    vals1 = df1.values.tolist()\n",
    "    vals2 = df2.values.tolist()\n",
    "    # Initialize numerator & denominator counters\n",
    "    compl_count = 0\n",
    "    real_total = df1.size\n",
    "    # Loop through values\n",
    "    for i in range(0, len(vals1)):\n",
    "        row = vals1[i]\n",
    "        for j in range(0, len(row)):\n",
    "            cell1 = row[j]\n",
    "            # Verify if cell1 is contained in cell2 (completely identified)\n",
    "            try:\n",
    "                cell2 = vals2[i][j]\n",
    "                if cell1 in cell2:\n",
    "                    compl_count += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "    # Calc\n",
    "    completeness_score = compl_count / real_total\n",
    "    return round(completeness_score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric: Purity\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\frac{\\text{\\# of purely detected elements}}{\\text{\\# of detected elements}}\n",
    "$$\n",
    "\n",
    "Explanation: When parts of multiple elements from the true table get merged into one, the detected output elements is classified as impure. Thus, a pure element is one whose components belong to only one original element. So it essentially looks to capture improperly metrged and false table elements.\n",
    "\n",
    "Scale:  $0 \\leq x \\leq 1$. It should be interpreted as a percentile score where a score of 1 (or 100%) indicates a perfectly pure processed table that in each cell, contains ONLY the items of the original table's respective cells. Meanwhile, a lower score near zero indicates the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity(df1, df2):\n",
    "    # Transform tables to lists of lists\n",
    "    vals1 = df1.values.tolist()\n",
    "    vals2 = df2.values.tolist()\n",
    "    # Initialize numerator & denominator counters\n",
    "    pure_count = 0\n",
    "    detected_total = df2.size\n",
    "    # Loop through values\n",
    "    for i in range(0, len(vals2)):\n",
    "        row = vals2[i]\n",
    "        for j in range(0, len(row)):\n",
    "            cell2 = row[j]\n",
    "            # Verify if cell2 is contained in cell1 (purely detected)\n",
    "            try:\n",
    "                cell1 = vals1[i][j]\n",
    "                if cell2 in cell1:\n",
    "                    pure_count += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "    # Calc\n",
    "    purity_score = pure_count / detected_total\n",
    "    return round(purity_score, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Location***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric: Precision\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\frac{\\text{\\# of correct proto-links}}{\\text{\\# of detected proto-links}}\n",
    "$$\n",
    "\n",
    "Explanation: A proto-link (PL) is defined as a connection between two neighbouring cells within a table. This metric looks to quantify proper cell location by measuring cell pairs that continue to share a border through the processed table output. It is similar to the recall metric but the ratio is over a different numerator, the total detected proto-links rather than total real proto-links. It is essentially calculated as the mirrored version of the recall metric.\n",
    "\n",
    "Scale:  $0 \\leq x \\leq 1$. This metric behaves like a similarity percentage with a score of 1 (100%) indicating completely identical tables while a score of 0 demonstrates entirely dissimilar tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(df1, df2):\n",
    "    # Transform tables to lists of lists\n",
    "    vals1 = df1.values.tolist()\n",
    "    vals2 = df2.values.tolist()\n",
    "    r,c = df2.shape\n",
    "    # Initialize numerator & denominator counters\n",
    "    correct_pl = 0\n",
    "    detected_pl = 2*r*c - r - c\n",
    "    # Loop through connections within rows\n",
    "    for i in range(0, len(vals2)):\n",
    "        row = vals2[i]\n",
    "        for j in range(0, len(row)-1):\n",
    "            cells2 = row[j:j+2]\n",
    "            # Verify if pair of cells2 is equal to pair of cells1 (correct PL)\n",
    "            try:\n",
    "                cells1 = vals1[i][j:j+2]\n",
    "                if cells2 == cells1:\n",
    "                    correct_pl += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "    # Loop through connections within columns\n",
    "    tvals1 = df1.transpose().values.tolist()\n",
    "    tvals2 = df2.transpose().values.tolist()\n",
    "    for i in range(0, len(tvals2)):\n",
    "        col = tvals2[i]\n",
    "        for j in range(0, len(col)-1):\n",
    "            cells2 = col[j:j+2]\n",
    "            # Verify if pair of cells2 is equal to pair of cells1 (correct PL)\n",
    "            try:\n",
    "                cells1 = tvals1[i][j:j+2]\n",
    "                if cells2 == cells1:\n",
    "                    correct_pl += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "    # Calc\n",
    "    precision_score = correct_pl / detected_pl\n",
    "    return round(precision_score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric: Recall\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\frac{\\text{\\# of correct proto-links}}{\\text{\\# of total proto-links}}\n",
    "$$\n",
    "\n",
    "Explanation: A proto-link (PL) is defined as a connection between two neighbouring cells within a table. This metric looks to quantify proper cell location by measuring cell pairs that continue to share a border through the processed table output. It is similar to the precision metric but the ratio is over a different numerator, the total real proto-links rather than total detected proto-links. It is essentially calculated as the mirrored version of the precision metric.\n",
    "\n",
    "Scale:  $0 \\leq x \\leq 1$. This metric behaves like a similarity percentage with a score of 1 (100%) indicating completely identical tables while a score of 0 demonstrates entirely dissimilar tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(df1, df2):\n",
    "    # Transform tables to lists of lists\n",
    "    vals1 = df1.values.tolist()\n",
    "    vals2 = df2.values.tolist()\n",
    "    r,c = df1.shape\n",
    "    # Initialize numerator & denominator counters\n",
    "    correct_pl = 0\n",
    "    total_pl = 2*r*c - r - c\n",
    "    # Loop through connections within rows\n",
    "    for i in range(0, len(vals1)):\n",
    "        row = vals1[i]\n",
    "        for j in range(0, len(row)-1):\n",
    "            cells1 = row[j:j+2]\n",
    "            # Verify if pair of cells1 is equal to pair of cells2 (correct PL)\n",
    "            try:\n",
    "                cells2 = vals2[i][j:j+2]\n",
    "                if cells1 == cells2:\n",
    "                    correct_pl += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "    # Loop through connections within columns\n",
    "    tvals1 = df1.transpose().values.tolist()\n",
    "    tvals2 = df2.transpose().values.tolist()\n",
    "    for i in range(0, len(tvals1)):\n",
    "        col = tvals1[i]\n",
    "        for j in range(0, len(col)-1):\n",
    "            cells1 = col[j:j+2]\n",
    "            # Verify if pair of cells1 is equal to pair of cells2 (correct PL)\n",
    "            try:\n",
    "                cells2 = tvals2[i][j:j+2]\n",
    "                if cells1 == cells2:\n",
    "                    correct_pl += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "    # Calc\n",
    "    recall_score = correct_pl / total_pl\n",
    "    return round(recall_score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Extraction Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all performance metrics for a table and its processed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_metrics(df1, df2):\n",
    "    # Clean & format dataframes\n",
    "    df1 = df1.fillna('').astype(str)\n",
    "    df2 = df2.fillna('').astype(str)\n",
    "    # Run all metric functions\n",
    "    results = {'Overlap':            overlap(df1, df2),\n",
    "               'String Similarity':  string_similarity(df1, df2), \n",
    "               'Completeness':       completeness(df1, df2),\n",
    "               'Purity':             purity(df1, df2),\n",
    "               'Precision':          precision(df1, df2),\n",
    "               'Recall':             recall(df1, df2)}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate all metrics for all table pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tables(tables):\n",
    "    # Initialize list to store metrics/results\n",
    "    results_lst = []\n",
    "    # Run all metrics for each table\n",
    "    for t in tables:\n",
    "        df1, df2 = tables[t]\n",
    "        results_lst.append(all_metrics(df1, df2))\n",
    "    # Return dataframe with all metrics (columns) for each table (rows)\n",
    "    return pd.DataFrame(results_lst, index=tables.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
